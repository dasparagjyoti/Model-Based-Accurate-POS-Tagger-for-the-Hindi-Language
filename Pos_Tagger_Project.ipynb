{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pos Tagger Project",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPYgHOxbvvkTcDxyy2SzCvI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dasparagjyoti/Model-Based-Accurate-POS-Tagger-for-the-Hindi-Language/blob/main/Pos_Tagger_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dQuJKBjLP3V",
        "outputId": "3b9a311b-73c4-479e-be14-59b860069e5a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('treebank')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aE4bQQPb2--"
      },
      "source": [
        "# **Accuracy Comparison**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaGIuMssrHV0"
      },
      "source": [
        "**Bigram Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulg4ZaZFb0Ln",
        "outputId": "987a8712-9a54-49af-a688-a0cb304a79e2"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# Importing the libraries and functions required\n",
        "from __future__ import division\n",
        "import codecs, time\n",
        "from collections import defaultdict\n",
        "from bigramViterbi import bigramHMMViterbiAlgorithm, calculateTransitionProbabilities, calculateEmissionProbabilities\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # CODE USED FOR CHECKING THE BEST VALUE OF LAMBDA AND OFFSET IS COMMENTED\n",
        "    # AND THE BEST VALUES ARE USED\n",
        "\n",
        "    # Lambdas = [0.00, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "    # Offsets = [1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001, 0.0000001, 0.00000001, 0.000000001, 0.0000000001, 0.00000000001, 0.000000000001]\n",
        "    # offset = 0.000001\n",
        "    Offsets = [1e-14]\n",
        "    # for i in range(5):\n",
        "        # Offsets.append(offset)\n",
        "        # offset /= 100\n",
        "    Lambdas = [0.04]\n",
        "    for Lambda in Lambdas:\n",
        "        for offset in Offsets:\n",
        "\n",
        "            # Opening the input training file\n",
        "            trainFile = codecs.open(\"trainDataHindi.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "            # Getting the emission probabilities\n",
        "            emissionProbabilityDict = calculateEmissionProbabilities(trainFile, Lambda, offset)\n",
        "\n",
        "            # Opening the input training file\n",
        "            trainFile = codecs.open(\"trainDataHindi.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "            # Getting the transition probabilities and all the tags (states)\n",
        "            transitionProbabilityDict, allTags = calculateTransitionProbabilities(trainFile, Lambda)\n",
        "\n",
        "            # Opening the input test file\n",
        "            testFile = codecs.open(\"testDataHindi.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "            # Calling the test function to get the accuracy of algorithm on the test data file\n",
        "            accuracy = testBigramHMMViterbiAlgorithm(testFile, allTags, emissionProbabilityDict, transitionProbabilityDict, Lambda)\n",
        "\n",
        "            # Printing the accuracy result\n",
        "            print(\"============================\")\n",
        "            print(\"ACCURACY : \" + str(accuracy) + \" %\" + \" \" + \"Offset : \" + str(offset) + \" \" + \"Lambda : \" + str(Lambda))\n",
        "            print(\"============================\")\n",
        "            # print(\"Offset : \" + str(offset))\n",
        "            # print(\"Lambda : \" + str(Lambda))\n",
        "            time.sleep(5)\n",
        "\n",
        "def testBigramHMMViterbiAlgorithm(testFile, allTags, emissionProbabilityDict, transitionProbabilityDict, Lambda):\n",
        "\n",
        "    # Initializing the variables required for calculating accuracy\n",
        "    totalTags = 0\n",
        "    correctTags = 0\n",
        "\n",
        "    # Reading the file line by line\n",
        "    for line in testFile.readlines():\n",
        "\n",
        "        # Getting tokens from each such line\n",
        "        tokens = line.split()\n",
        "\n",
        "        # Initializing a list for the words observed in the line\n",
        "        sentence = []\n",
        "\n",
        "        # Initializing a list for the tags observed in the line\n",
        "        tags = []\n",
        "\n",
        "        for token in tokens:\n",
        "\n",
        "            # Extracting the word by splitting and stripping according to the file\n",
        "            word = token.split('|')[0].strip()\n",
        "            # Extracting the tag by splitting and stripping according to the file\n",
        "            tag = token.split('|')[2].split('.')[0].strip(':?').strip()\n",
        "\n",
        "            # Giving exact tags in training data a common parent tag for less complexity\n",
        "            # and more accuracy\n",
        "            if (tag == 'I-NP' or tag == 'B-NP' or tag == 'O'):\n",
        "                tag = 'NN'\n",
        "\n",
        "            # Appending the word to the list of words\n",
        "            sentence.append(word)\n",
        "            # Appending the tag to the list of tags\n",
        "            tags.append(tag)\n",
        "\n",
        "        # If the line read is not a blank line\n",
        "        if sentence != []:\n",
        "\n",
        "            # Zipping to get a list of (word, tag) tuple elements format\n",
        "            actualWordsAndTags = zip(sentence, tags)\n",
        "\n",
        "            try:\n",
        "                # Calling the function on the sentence\n",
        "                predictedWordsAndTags = bigramHMMViterbiAlgorithm(sentence, allTags, emissionProbabilityDict, transitionProbabilityDict)\n",
        "\n",
        "                # Iterating through to compare the predicted and actual tags\n",
        "                for i in range(len(sentence)):\n",
        "                    actualTag = actualWordsAndTags[i][1]\n",
        "                    predictedTag = predictedWordsAndTags[i][1]\n",
        "\n",
        "                    # Incrementing the number of correct tags by 1 if it is such\n",
        "                    if actualTag == predictedTag:\n",
        "                        correctTags += 1\n",
        "\n",
        "                    # Incrementing the number of total tags by 1\n",
        "                    totalTags += 1\n",
        "\n",
        "                    # Printing the results in between the interations\n",
        "                    print(\"CORRECT TAGS : \" + str(correctTags) + \"  ======== \" ,)\n",
        "                    print(\"TOTAL TAGS : \" + str(totalTags))\n",
        "\n",
        "            # A couple of lines in the file are arbitrary\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    # Returning the result\n",
        "\n",
        "    return ()\n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of Words     : 396712\n",
            "Total number of Sentences : 16993\n",
            "============================\n",
            "ACCURACY : 92.8694 % Offset : 1e-14 Lambda : 0.04\n",
            "============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tIUj5vxti6C"
      },
      "source": [
        "**Trigram accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGbnYiZ2tycS",
        "outputId": "4b31c8b8-6555-4277-fc18-9ac823a3d45b"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import division\n",
        "import codecs, time\n",
        "from collections import defaultdict\n",
        "from trigramViterbi import trigramHMMViterbiAlgorithm, calculateTransitionProbabilities, calculateEmissionProbabilities\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # CODE USED FOR CHECKING THE BEST VALUE OF LAMBDA AND OFFSET IS COMMENTED\n",
        "    # AND THE BEST VALUES ARE USED\n",
        "\n",
        "    # Lambdas = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "        # Lambda = 0.0\n",
        "    # offset = 0.000001\n",
        "    Offsets = [1e-14]\n",
        "    # for i in range(6):\n",
        "    #     Offsets.append(offset)\n",
        "    #     offset /= 100\n",
        "    Lambdas = [0.0]\n",
        "    for Lambda in Lambdas:\n",
        "        for offset in Offsets:\n",
        "            # Opening the input training file\n",
        "            trainFile = codecs.open(\"trainDataHindi.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "            # Getting the emission probabilities\n",
        "            emissionProbabilityDict = calculateEmissionProbabilities(trainFile, Lambda, offset)\n",
        "\n",
        "            # Opening the input training file\n",
        "            trainFile = codecs.open(\"trainDataHindi.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "            # Getting the transition probabilities and all the tags (states)\n",
        "            transitionProbabilityDict, allTags = calculateTransitionProbabilities(trainFile, Lambda)\n",
        "\n",
        "            # Opening the input test file\n",
        "            testFile = codecs.open(\"testDataHindi.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "            # Calling the test function to get the accuracy of algorithm on the test data file\n",
        "            accuracy = testTrigramHMMViterbiAlgorithm(testFile, allTags, emissionProbabilityDict, transitionProbabilityDict)\n",
        "\n",
        "            # Printing the accuracy result\n",
        "            print(\"============================\")\n",
        "            print(\"ACCURACY : \" + str(accuracy) + \" %\" + \" \" + \"Offset : \" + str(offset) + \" \" + \"Lambda : \" + str(Lambda))\n",
        "            print(\"============================\")\n",
        "            time.sleep(1)\n",
        "\n",
        "def testTrigramHMMViterbiAlgorithm(testFile, allTags, emissionProbabilityDict, transitionProbabilityDict):\n",
        "\n",
        "    # Initializing the variables required for calculating accuracy\n",
        "    totalTags = 0\n",
        "    correctTags = 0\n",
        "\n",
        "    # Reading the file line by line\n",
        "    for line in testFile.readlines():\n",
        "\n",
        "        # Getting tokens from each such line\n",
        "        tokens = line.split()\n",
        "\n",
        "        # Initializing a list for the words observed in the line\n",
        "        sentence = []\n",
        "\n",
        "        # Initializing a list for the tags observed in the line\n",
        "        tags = []\n",
        "\n",
        "        for token in tokens:\n",
        "\n",
        "            # Extracting the word by splitting and stripping according to the file\n",
        "            word = token.split('|')[0].strip()\n",
        "            # Extracting the tag by splitting and stripping according to the file\n",
        "            tag = token.split('|')[2].split('.')[0].strip(':?').strip()\n",
        "\n",
        "            # Giving exact tags in training data a common parent tag for less complexity\n",
        "            # and more accuracy\n",
        "            if (tag == 'I-NP' or tag == 'B-NP' or tag == 'O'):\n",
        "                tag = 'NN'\n",
        "\n",
        "            # Appending the word to the list of words\n",
        "            sentence.append(word)\n",
        "            # Appending the tag to the list of tags\n",
        "            tags.append(tag)\n",
        "\n",
        "        # If the line read is not a blank line\n",
        "        if sentence != []:\n",
        "\n",
        "            # Zipping to get a list of (word, tag) tuple elements format\n",
        "            actualWordsAndTags = zip(sentence, tags)\n",
        "\n",
        "            try:\n",
        "                # Calling the function on the sentence\n",
        "                predictedWordsAndTags = trigramHMMViterbiAlgorithm(sentence, allTags, emissionProbabilityDict, transitionProbabilityDict)\n",
        "\n",
        "                # Iterating through to compare the predicted and actual tags\n",
        "                for i in range(len(sentence)):\n",
        "                    actualTag = actualWordsAndTags[i][1]\n",
        "                    predictedTag = predictedWordsAndTags[i][1]\n",
        "\n",
        "                    # Incrementing the number of correct tags by 1 if it is such\n",
        "                    if actualTag == predictedTag:\n",
        "                        correctTags += 1\n",
        "\n",
        "                    # Incrementing the number of total tags by 1\n",
        "                    totalTags += 1\n",
        "\n",
        "                    # Printing the results in between the interations\n",
        "                    print(\"CORRECT TAGS : \" + str(correctTags) + \"  ======== \" ,)\n",
        "                    print(\"TOTAL TAGS : \" + str(totalTags))\n",
        "\n",
        "            # A couple of lines in the file are arbitrary\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    # Returning the result\n",
        "    return ()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Words     : 396712\n",
            "Number of Sentences : 16993\n",
            "============================\n",
            "ACCURACY : 84.9796 % Offset : 1e-14 Lambda : 0.0\n",
            "============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ0KZSUwuFBu"
      },
      "source": [
        "**Improved Trigram Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Vahj-kAuRFd",
        "outputId": "cea662e6-8ef4-4d56-e7b7-2bffcf91f743"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import division\n",
        "import codecs, time\n",
        "from collections import defaultdict\n",
        "from trigramViterbi import trigramHMMViterbiAlgorithm, calculateTransitionProbabilities, calculateEmissionProbabilities\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # CODE USED FOR CHECKING THE BEST VALUE OF LAMBDA AND OFFSET IS COMMENTED\n",
        "    # AND THE BEST VALUES ARE USED\n",
        "\n",
        "    # Lambdas = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "        # Lambda = 0.0\n",
        "    # offset = 0.000001\n",
        "    Offsets = [1e-14]\n",
        "    # for i in range(6):\n",
        "    #     Offsets.append(offset)\n",
        "    #     offset /= 100\n",
        "    Lambdas = [0.0]\n",
        "    for Lambda in Lambdas:\n",
        "        for offset in Offsets:\n",
        "            # Opening the input training file\n",
        "            trainFile = codecs.open(\"trainDataHindi.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "            # Getting the emission probabilities\n",
        "            emissionProbabilityDict = calculateEmissionProbabilities(trainFile, Lambda, offset)\n",
        "\n",
        "            # Opening the input training file\n",
        "            trainFile = codecs.open(\"trainDataHindi.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "            # Getting the transition probabilities and all the tags (states)\n",
        "            transitionProbabilityDict, allTags = calculateTransitionProbabilities(trainFile, Lambda)\n",
        "\n",
        "            # Opening the input test file\n",
        "            testFile = codecs.open(\"testDataHindi.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "            # Calling the test function to get the accuracy of algorithm on the test data file\n",
        "            accuracy = testTrigramHMMViterbiAlgorithm(testFile, allTags, emissionProbabilityDict, transitionProbabilityDict)\n",
        "\n",
        "            # Printing the accuracy result\n",
        "            print(\"============================\")\n",
        "            print(\"ACCURACY : \" + str(accuracy) + \" %\" + \" \" + \"Offset : \" + str(offset) + \" \" + \"Lambda : \" + str(Lambda))\n",
        "            print(\"============================\")\n",
        "            time.sleep(1)\n",
        "\n",
        "def testTrigramHMMViterbiAlgorithm(testFile, allTags, emissionProbabilityDict, transitionProbabilityDict):\n",
        "\n",
        "    # Initializing the variables required for calculating accuracy\n",
        "    totalTags = 0\n",
        "    correctTags = 0\n",
        "\n",
        "    # Reading the file line by line\n",
        "    for line in testFile.readlines():\n",
        "\n",
        "        # Getting tokens from each such line\n",
        "        tokens = line.split()\n",
        "\n",
        "        # Initializing a list for the words observed in the line\n",
        "        sentence = []\n",
        "\n",
        "        # Initializing a list for the tags observed in the line\n",
        "        tags = []\n",
        "\n",
        "        for token in tokens:\n",
        "\n",
        "            # Extracting the word by splitting and stripping according to the file\n",
        "            word = token.split('|')[0].strip()\n",
        "            # Extracting the tag by splitting and stripping according to the file\n",
        "            tag = token.split('|')[2].split('.')[0].strip(':?').strip()\n",
        "\n",
        "            # Giving exact tags in training data a common parent tag for less complexity\n",
        "            # and more accuracy\n",
        "            if (tag == 'I-NP' or tag == 'B-NP' or tag == 'O'):\n",
        "                tag = 'NN'\n",
        "\n",
        "            # Appending the word to the list of words\n",
        "            sentence.append(word)\n",
        "            # Appending the tag to the list of tags\n",
        "            tags.append(tag)\n",
        "\n",
        "        # If the line read is not a blank line\n",
        "        if sentence != []:\n",
        "\n",
        "            # Zipping to get a list of (word, tag) tuple elements format\n",
        "            actualWordsAndTags = zip(sentence, tags)\n",
        "\n",
        "            try:\n",
        "                # Calling the function on the sentence\n",
        "                predictedWordsAndTags = trigramHMMViterbiAlgorithm(sentence, allTags, emissionProbabilityDict, transitionProbabilityDict)\n",
        "\n",
        "                # Iterating through to compare the predicted and actual tags\n",
        "                for i in range(len(sentence)):\n",
        "                    actualTag = actualWordsAndTags[i][1]\n",
        "                    predictedTag = predictedWordsAndTags[i][1]\n",
        "\n",
        "                    # Incrementing the number of correct tags by 1 if it is such\n",
        "                    if actualTag == predictedTag:\n",
        "                        correctTags += 1\n",
        "\n",
        "                    # Incrementing the number of total tags by 1\n",
        "                    totalTags += 1\n",
        "\n",
        "                    # Printing the results in between the interations\n",
        "                    print(\"CORRECT TAGS : \" + str(correctTags) + \"  ======== \" ,)\n",
        "                    print(\"TOTAL TAGS : \" + str(totalTags))\n",
        "\n",
        "            # A couple of lines in the file are arbitrary\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    # Returning the result\n",
        "    return ()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Words     : 396712\n",
            "Number of Sentences : 16993\n",
            "============================\n",
            "ACCURACY : 88.071 % Offset : 1e-14 Lambda : 0.0\n",
            "============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-GRsqd_cQDV"
      },
      "source": [
        "# **Result Comparasion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGgt7ldnaJxQ"
      },
      "source": [
        "**Result of Bigram HMM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cTOyGqkZuIn",
        "outputId": "667b8756-38a0-4223-9b42-27e83d4f45a9"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import division\n",
        "import codecs\n",
        "from collections import defaultdict\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Opening the input training file\n",
        "    trainFile = codecs.open(\"trainDataHindi.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "    # Getting the emission probabilities\n",
        "    emissionProbabilityDict = calculateEmissionProbabilities(trainFile, 0.0, 0.000000001)\n",
        "\n",
        "    # Opening the input training file\n",
        "    trainFile = codecs.open(\"trainDataHindi.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "    # Getting the transition probabilities and all the tags (states)\n",
        "    transitionProbabilityDict, allTags = calculateTransitionProbabilities(trainFile, 0.0)\n",
        "\n",
        "    # Opening the input file\n",
        "    inputSentences = codecs.open(\"input.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "    # Initializing a list for sentences\n",
        "    sentencesList = []\n",
        "\n",
        "    # Reading the input file and storing the sentences\n",
        "    for line in inputSentences:\n",
        "        sentence = []\n",
        "        tokens = line.split()\n",
        "        for token in tokens:\n",
        "            word = token.split('|')[0].strip()\n",
        "            sentence.append(token)\n",
        "\n",
        "        # Appending the sentence in the list of sentences\n",
        "        sentencesList.append(sentence)\n",
        "\n",
        "    print(\"\\n===========================\\nPRINTING THE RESULTS\\n===========================\\n\")\n",
        "    # Iterating through each of the sentences\n",
        "    for sentence in sentencesList:\n",
        "\n",
        "        # Calling the algorithm on the sentence\n",
        "        predictedWordsAndTags = bigramHMMViterbiAlgorithm(sentence, allTags, emissionProbabilityDict, transitionProbabilityDict)\n",
        "        print(\"\\n=======\\n\")\n",
        "        # Printing the result of the algorithm\n",
        "        for (word, tag) in predictedWordsAndTags:\n",
        "            print(word, tag,)\n",
        "        print(\"\\n\")\n",
        "    print(\"\\n=========================\\n\")\n",
        "\n",
        "def bigramHMMViterbiAlgorithm(sentence, allTags, emissionProbabilityDict, transitionProbabilityDict):\n",
        "\n",
        "    # Appending a '*' required for the initial bigram in the algorithm(base case)\n",
        "    sentence = [ u'*'] + sentence\n",
        "\n",
        "    # Declaring the variables required\n",
        "    initialLength = len(sentence)\n",
        "    backtrackingDict = defaultdict(list)\n",
        "    dpDict = {}\n",
        "    tagsAssigned = []\n",
        "\n",
        "    # Initializing the base cases\n",
        "    dpDict[0, '*'] = 1\n",
        "    for u in ([ u'*' ] + list(allTags)):\n",
        "            if u != u'*':\n",
        "                dpDict[0, u] = 1\n",
        "\n",
        "    # Iterating through each word in the sentence from starting in terms of length\n",
        "    for k in range(1, initialLength):\n",
        "        # For all possibilities of (u, v) tags\n",
        "        for v in allTags:\n",
        "\n",
        "            # List for possibilities for getting maximum probability\n",
        "            possibilities = []\n",
        "\n",
        "            # For all possibilities of (u, v) tags\n",
        "            for u in allTags:\n",
        "\n",
        "                # Viterbi algorithm formula\n",
        "                possibility = dpDict[k - 1 , u] * transitionProbabilityDict[(v, u)] * (emissionProbabilityDict[sentence[k] + '|' + v])\n",
        "\n",
        "                # Appending the possibility to the list of possibilities\n",
        "                possibilities.append((possibility, u))\n",
        "\n",
        "            # Getting the maximum probability from the list of possibilities, element[0] is the probability of the possibility\n",
        "            maxUVgivenK = max(possibilities, key = lambda element : element[0])\n",
        "\n",
        "            # Storing the answer in the DP dict\n",
        "            dpDict[k, v] = maxUVgivenK[0]\n",
        "\n",
        "            # Getting the element to be backtracked and storing it in the backtracking Dict\n",
        "            backtrackU = maxUVgivenK[1]\n",
        "            backtrackingDict[v] += [backtrackU]\n",
        "\n",
        "            # Additionally storing v of (u, v) tags in the last iteration\n",
        "            if k == initialLength - 1:\n",
        "                backtrackingDict[v] += [v]\n",
        "\n",
        "\n",
        "    # List for getting the possibilities of final iterations\n",
        "    maxPossibilitiesList = []\n",
        "\n",
        "    for key in dpDict:\n",
        "\n",
        "        # Getting the possibilities of the final iterations and appending to maxPossibilitiesList\n",
        "        if (initialLength - 1 in key) and (dpDict[key] != 0):\n",
        "            maxPossibilitiesList.append((key[1], dpDict[key]))\n",
        "\n",
        "    # Finally getting the answer of the viterbi alogrithm, element[1] is the probability of the final iteration possibilities\n",
        "    viterbiAnswer = max(maxPossibilitiesList, key = lambda element:element[1])\n",
        "\n",
        "    # Getting the tags associated with the viterbiAnswer by referring to the backtrackingDP\n",
        "    tagsAssigned = backtrackingDict[viterbiAnswer[0]][1:]\n",
        "\n",
        "    # Removing the '*' appended in the starting of algorithm, not required anymore\n",
        "    sentence = sentence[1:]\n",
        "\n",
        "    # Returning the answer\n",
        "    return zip(sentence, tagsAssigned)\n",
        "\n",
        "\n",
        "def calculateTransitionProbabilities(trainFile, Lambda):\n",
        "\n",
        "    # Declaring the variables and default dicts required\n",
        "    transitionProbabilityDict = defaultdict(int)\n",
        "    bigramTransitionCountDict = defaultdict(int)\n",
        "    unigramTransitionCountDict = defaultdict(int)\n",
        "    numberOfWords = 0\n",
        "    numberOfSentences = 0\n",
        "    allTags = set([])\n",
        "\n",
        "    # Iterating through the lines of the input file\n",
        "    for line in trainFile.readlines():\n",
        "\n",
        "        # Getting tokens from each such line\n",
        "        tokens = line.split()\n",
        "\n",
        "        # Initializing a list for the tags observed in the line\n",
        "        tags = []\n",
        "\n",
        "        # For each token in that line\n",
        "        for token in tokens:\n",
        "\n",
        "            # Extracting the tag by splitting and stripping according to the file\n",
        "            tag = token.split('|')[2].split('.')[0].strip(':?').strip()\n",
        "\n",
        "            # Giving exact tags in training data a common parent tag for less complexity\n",
        "            # and more accuracy\n",
        "            if (tag == 'I-NP' or tag == 'B-NP' or tag == 'O'):\n",
        "                tag = 'NN'\n",
        "\n",
        "            allTags = allTags | set([tag])\n",
        "\n",
        "            # Appending the tag to the list of tags\n",
        "            tags.append(tag)\n",
        "\n",
        "        # Incrementing the total number of words by the required amount\n",
        "        numberOfWords += len(tags)\n",
        "\n",
        "        # If the line read is not a blank line\n",
        "        if tags != []:\n",
        "\n",
        "            # Incrementing the total number of sentences by the required amount\n",
        "            numberOfSentences += 1\n",
        "\n",
        "            # Getting the unigrams in the exact order\n",
        "            unigramSentenceList = tags[:]\n",
        "\n",
        "            # Getting the bigrams in the exact order by zipping through the tags\n",
        "            bigramSentenceList = zip(tags, tags[1:])\n",
        "\n",
        "            # Incrementing the count of the unigrams as observed in the unigramSentenceList\n",
        "            for unigram in unigramSentenceList:\n",
        "                unigramTransitionCountDict[unigram] += 1\n",
        "\n",
        "            # Incrementing the count of the unigrams as observed in the bigramSentenceList\n",
        "            for bigram in bigramSentenceList:\n",
        "                bigramTransitionCountDict[bigram] += 1\n",
        "\n",
        "    # Calculating the transition probabilities finally\n",
        "    # Iterating through the (u, v) bigrams\n",
        "    for bigram in bigramTransitionCountDict:\n",
        "\n",
        "        # Initializing variables as required\n",
        "        uUnigram = bigram[0]\n",
        "        vState = bigram[1]\n",
        "\n",
        "        # Declaring the q(v | u) - transition probability\n",
        "        keyVgivenU = (vState, uUnigram)\n",
        "\n",
        "        # Calculating the transition probability\n",
        "        transitionProbability = (bigramTransitionCountDict[bigram] + Lambda ) / (unigramTransitionCountDict[uUnigram] + (Lambda * (22 ** 2)))\n",
        "\n",
        "        # Storing the transition probability in the dict\n",
        "        transitionProbabilityDict[keyVgivenU] = transitionProbability\n",
        "\n",
        "    # Printing the total number of words and sentences\n",
        "    print(\"Total number of Words     : \" + str(numberOfWords))\n",
        "    print(\"Total number of Sentences : \" + str(numberOfSentences))\n",
        "\n",
        "    # Returning probability dict and tags set\n",
        "    return transitionProbabilityDict, allTags\n",
        "\n",
        "\n",
        "def calculateEmissionProbabilities(trainFile, Lambda, offset):\n",
        "\n",
        "    # Declaring the variables and default dicts required\n",
        "    # 0.000000001 as offset for smoothing\n",
        "    emissionProbabilityDict = defaultdict(lambda : offset)\n",
        "    emissionCountDict = defaultdict(int)\n",
        "    separateTagCountDict = defaultdict(int)\n",
        "\n",
        "    # Iterating through the lines of the input file\n",
        "    for line in trainFile.readlines():\n",
        "\n",
        "        # Getting tokens form each such line\n",
        "        tokens = line.split()\n",
        "\n",
        "        # Initializing a list for the tags observed in the line\n",
        "        tags = []\n",
        "\n",
        "        # For each token in that line\n",
        "        for token in tokens:\n",
        "\n",
        "                # Extracting the word by splitting and stripping according to the file\n",
        "                word  = token.split('|')[0].strip()\n",
        "                # Extracting the tag by splitting and stripping according to the file\n",
        "                tag = token.split('|')[2].split('.')[0].strip(':?').strip()\n",
        "\n",
        "                # Giving exact tags in training data a common parent tag for less complexity\n",
        "                # and more accuracy\n",
        "                if (tag == 'I-NP' or tag == 'B-NP' or tag == 'O'):\n",
        "                    tag = 'NN'\n",
        "\n",
        "                # Appending the tag to the list of tags\n",
        "                tags.append(tag)\n",
        "\n",
        "                # Calculating the number of times the tag and word appear together\n",
        "                emissionCountDict[tag + '|' + word] += 1\n",
        "\n",
        "                # Calculating the number of times that tag appears in general\n",
        "                separateTagCountDict[tag] += 1\n",
        "\n",
        "    # Iterating through the tag and word in the keys of emissionCountDict\n",
        "    for key in emissionCountDict:\n",
        "\n",
        "        # Getting the tag and word from the key\n",
        "        tagAndWord = key.split('|')\n",
        "        tag = tagAndWord[0]\n",
        "        word = tagAndWord[1]\n",
        "\n",
        "        # Calculating the emission probability\n",
        "        emissionProbability = (emissionCountDict[tag + '|' + word] + Lambda ) / (separateTagCountDict[tag] + (Lambda * (22 ** 2)))\n",
        "\n",
        "        # Storing the emission probability in the dict\n",
        "        emissionProbabilityDict[word + '|' + tag] = emissionProbability\n",
        "\n",
        "    # Returning probability dict\n",
        "    return emissionProbabilityDict\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of Words     : 396712\n",
            "Total number of Sentences : 16993\n",
            "\n",
            "===========================\n",
            "PRINTING THE RESULTS\n",
            "===========================\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "साइबेरिया NN\n",
            "में PSP\n",
            "जनसँख्या NN\n",
            "का PSP\n",
            "औसत JJ\n",
            "घनत्व NN\n",
            "केवल RP\n",
            "4 QC\n",
            "व्यक्ति NN\n",
            "प्रति PSP\n",
            "वर्ग NN\n",
            "किलोमीटर NN\n",
            "है VM\n",
            "\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "उत्तरी JJ\n",
            "साइबेरिया NN\n",
            "बहुत QF\n",
            "सर्द VM\n",
            "क्षेत्र NN\n",
            "है VM\n",
            "और CC\n",
            "यहाँ PRP\n",
            "गरमी VM\n",
            "का PSP\n",
            "मौसम NN\n",
            "केवल RP\n",
            "एक QC\n",
            "महीने NN\n",
            "रहता VM\n",
            "है VAUX\n",
            "\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "तुलना NN\n",
            "के PSP\n",
            "लिए PSP\n",
            "सन् XC\n",
            "2011 NN\n",
            "की PSP\n",
            "जनगणना NN\n",
            "में PSP\n",
            "भारत NNP\n",
            "के PSP\n",
            "बिहार NNP\n",
            "राज्य NN\n",
            "में PSP\n",
            "जन-घनत्व NN\n",
            "1102 PSP\n",
            "व्यक्ति NN\n",
            "प्रति PSP\n",
            "वर्ग NN\n",
            "किमी NN\n",
            "था VM\n",
            "\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "मैं PRP\n",
            "थक VM\n",
            "गया VAUX\n",
            "हूँ VAUX\n",
            "\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "आपका PRP\n",
            "नाम NN\n",
            "क्या WQ\n",
            "है VM\n",
            "\n",
            "\n",
            "\n",
            "=========================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z43Aj48ec6qX"
      },
      "source": [
        "**Result of Trigram HMM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4wSHLt_dE74",
        "outputId": "77a106a8-1058-4f3f-a49c-a5b65c4237a5"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import division\n",
        "import codecs\n",
        "from collections import defaultdict\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Opening the input training file\n",
        "    trainFile = codecs.open(\"trainDataHindi.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "    # Getting the emission probabilities\n",
        "    emissionProbabilityDict = calculateEmissionProbabilities(trainFile, 0.0, 0.000000001)\n",
        "\n",
        "    # Opening the input training file\n",
        "    trainFile = codecs.open(\"trainDataHindi.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "    # Getting the transition probabilities and all the tags (states)\n",
        "    transitionProbabilityDict, allTags = calculateTransitionProbabilities(trainFile, 0.0)\n",
        "\n",
        "    # Opening the input file\n",
        "    inputSentences = codecs.open(\"input.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "    # Initializing a list for sentences\n",
        "    sentencesList = []\n",
        "\n",
        "    # Reading the input file and storing the sentences\n",
        "    for line in inputSentences:\n",
        "        sentence = []\n",
        "        tokens = line.split()\n",
        "        for token in tokens:\n",
        "            word = token.split('|')[0].strip()\n",
        "            sentence.append(token)\n",
        "\n",
        "        # Appending the sentence in the list of sentences\n",
        "        sentencesList.append(sentence)\n",
        "\n",
        "    print(\"\\n===========================\\nPRINTING THE RESULTS\\n===========================\\n\")\n",
        "    # Iterating through each of the sentences\n",
        "    for sentence in sentencesList:\n",
        "\n",
        "        # Calling the algorithm on the sentence\n",
        "        predictedWordsAndTags = trigramHMMViterbiAlgorithm(sentence, allTags, emissionProbabilityDict, transitionProbabilityDict)\n",
        "        print(\"\\n=======\\n\")\n",
        "        # Printing the result of the algorithm\n",
        "        for (word, tag) in predictedWordsAndTags:\n",
        "            print(word, tag,)\n",
        "        print(\"\\n\")\n",
        "    print(\"\\n=========================\\n\")\n",
        "\n",
        "def trigramHMMViterbiAlgorithm(sentence, allTags, emissionProbabilityDict, transitionProbabilityDict):\n",
        "\n",
        "    # Appending two '*'s required for the initial trigram in the algorithm(base case)\n",
        "    sentence = [ u'*', u'*' ] + sentence\n",
        "\n",
        "    # Declaring the variables required\n",
        "    initialLength = len(sentence)\n",
        "    backtrackingDict = defaultdict(list)\n",
        "    dpDict = {}\n",
        "    tagsAssigned = []\n",
        "\n",
        "    # Initializing the base cases\n",
        "    dpDict[0, '*', '*'] = 1\n",
        "    for u in ([ u'*' ] + list(allTags)):\n",
        "        for v in ([ u'*' ] + list(allTags)):\n",
        "            if u != u'*' or v != u'*':\n",
        "                dpDict[0, u, v] = 1\n",
        "\n",
        "    # Iterating through each word in the sentence from starting in terms of length\n",
        "    for k in range(1, initialLength):\n",
        "        # For all possibilities of (w, u, v) tags\n",
        "        for v in allTags:\n",
        "            # For all possibilities of (w, u, v) tags\n",
        "            for u in allTags:\n",
        "\n",
        "                # List for possibilities for getting maximum probability\n",
        "                possibilities = []\n",
        "\n",
        "                # For all possibilities of (w, u, v) tags\n",
        "                for w in allTags:\n",
        "\n",
        "                    # Viterbi algorithm formula\n",
        "                    possibility = dpDict[k - 1 , w, u] * transitionProbabilityDict[(v, w, u)] * (emissionProbabilityDict[sentence[k] + '|' + v])\n",
        "\n",
        "                    # Appending the possibility to the list of possibilities\n",
        "                    possibilities.append((possibility, w))\n",
        "\n",
        "                # Getting the maximum probability from the list of possibilities, element[0] is the probability of the possibility\n",
        "                maxWUVgivenK = max(possibilities, key = lambda element:element[0])\n",
        "\n",
        "                # Storing the answer in the DP dict\n",
        "                dpDict[k, u, v] = maxWUVgivenK[0]\n",
        "\n",
        "                # Getting the element to be backtracked and storing it in the backtracking Dict\n",
        "                backtrackW = maxWUVgivenK[1]\n",
        "                backtrackingDict[u, v] += [backtrackW]\n",
        "\n",
        "                # Additionally storing (u, v) of (w, u, v) tags in the last iteration\n",
        "                if k == initialLength - 1:\n",
        "                    backtrackingDict[u, v] += [u, v]\n",
        "\n",
        "    # List for getting the possibilities of final iterations\n",
        "    maxPossibilitiesList = []\n",
        "\n",
        "    for key in dpDict:\n",
        "\n",
        "        # Getting the possibilities of the final iterations and appending to maxPossibilitiesList\n",
        "        if (initialLength - 1 in key) and dpDict[key]!=0:\n",
        "            maxPossibilitiesList.append((key[1:], dpDict[key]))\n",
        "\n",
        "    # Finally getting the answer of the viterbi alogrithm, element[1] is the probability of the final iteration possibilities\n",
        "    viterbiAnswer = max(maxPossibilitiesList, key = lambda element:element[1])\n",
        "\n",
        "    # Getting the tags associated with the viterbiAnswer by referring to the backtrackingDict\n",
        "    tagsAssigned = backtrackingDict[viterbiAnswer[0]][3:]\n",
        "\n",
        "    # Removing the two '*'s appended in the starting of algorithm, not required anymore\n",
        "    sentence = sentence[2:]\n",
        "\n",
        "    # Returning the answer\n",
        "    return zip(sentence, tagsAssigned)\n",
        "\n",
        "\n",
        "def calculateTransitionProbabilities(trainFile, Lambda):\n",
        "\n",
        "    # Declaring the variables and default dicts required\n",
        "    transitionProbabilityDict = defaultdict(int)\n",
        "    trigramTransitionCountDict = defaultdict(int)\n",
        "    bigramTransitionCountDict = defaultdict(int)\n",
        "    numberOfWords = 0\n",
        "    numberOfSentences = 0\n",
        "    allTags = set([])\n",
        "\n",
        "    # Iterating through the lines of the input file\n",
        "    for line in trainFile.readlines():\n",
        "\n",
        "        # Getting tokens from each such line\n",
        "        tokens = line.split()\n",
        "\n",
        "        # Initializing a list for the tags observed in the line\n",
        "        tags = []\n",
        "\n",
        "        # For each token in that line\n",
        "        for token in tokens:\n",
        "\n",
        "            # Extracting the tag by splitting and stripping according to the file\n",
        "            tag = token.split('|')[2].split('.')[0].strip(':?').strip()\n",
        "\n",
        "            # Giving exact tags in training data a common parent tag for less complexity\n",
        "            # and more accuracy\n",
        "            if (tag == 'I-NP' or tag == 'B-NP' or tag == 'O'):\n",
        "                tag = 'NN'\n",
        "\n",
        "            allTags = allTags | set([tag])\n",
        "\n",
        "            # Appending the tag to the list of tags\n",
        "            tags.append(tag)\n",
        "\n",
        "        # Incrementing the total number of words by the required amount\n",
        "        numberOfWords += len(tags)\n",
        "\n",
        "        # If the line read is not a blank line\n",
        "        if tags != []:\n",
        "\n",
        "            # Incrementing the total number of sentences by the required amount\n",
        "            numberOfSentences += 1\n",
        "\n",
        "            # Getting the bigrams in the exact order by zipping through the tags\n",
        "            bigramSentenceList = zip(tags,tags[1:])\n",
        "\n",
        "            # Getting the trigrams in the exact order by zipping through the tags\n",
        "            trigramSentenceList = zip(tags, tags[1:], tags[2:])\n",
        "\n",
        "            # Incrementing the count of the bigrams as observed in the bigramSentenceList\n",
        "            for bigram in bigramSentenceList:\n",
        "                bigramTransitionCountDict[bigram] += 1\n",
        "\n",
        "            # Incrementing the count of the trigrams as observed in the trigramSentenceList\n",
        "            for trigram in trigramSentenceList:\n",
        "                trigramTransitionCountDict[trigram] += 1\n",
        "\n",
        "    # Calculating the transition probabilities finally\n",
        "    # Iterating through the (u, v, s) trigrams\n",
        "    for trigram in trigramTransitionCountDict:\n",
        "\n",
        "        # Initializing variables as required\n",
        "        uvBigram = trigram[:-1]\n",
        "        sState = (trigram[-1],)\n",
        "\n",
        "        # Declaring the q(s | u, v) - transition probability\n",
        "        keySgivenUV = sState + uvBigram\n",
        "\n",
        "        # Calculating the transition probability\n",
        "        transitionProbability = (trigramTransitionCountDict[trigram] + Lambda) / (bigramTransitionCountDict[uvBigram] + (Lambda * (22 ** 2)))\n",
        "\n",
        "        # Storing the transition probability in the dict\n",
        "        transitionProbabilityDict[keySgivenUV] = transitionProbability\n",
        "\n",
        "    # Printing the total number of words and sentences\n",
        "    print(\"Number of Words     : \" + str(numberOfWords))\n",
        "    print(\"Number of Sentences : \" + str(numberOfSentences))\n",
        "\n",
        "    # Returning probability dict and tags set\n",
        "    return transitionProbabilityDict, allTags\n",
        "\n",
        "\n",
        "def calculateEmissionProbabilities(trainFile, Lambda, offset):\n",
        "\n",
        "    # Declaring the variables and default dicts required\n",
        "    # 0.000000001 as offset for smoothing\n",
        "    emissionProbabilityDict = defaultdict(lambda :  offset)\n",
        "    emissionCountDict = defaultdict(int)\n",
        "    separateTagCountDict = defaultdict(int)\n",
        "\n",
        "    # Iterating through the lines of the input file\n",
        "    for line in trainFile.readlines():\n",
        "\n",
        "        # Getting tokens form each such line\n",
        "        tokens = line.split()\n",
        "\n",
        "        # Initializing a list for the tags observed in the line\n",
        "        tags = []\n",
        "\n",
        "        # For each token in that line\n",
        "        for token in tokens:\n",
        "\n",
        "                # Extracting the word by splitting and stripping according to the file\n",
        "                word  = token.split('|')[0].strip()\n",
        "                # Extracting the tag by splitting and stripping according to the file\n",
        "                tag = token.split('|')[2].split('.')[0].strip(':?').strip()\n",
        "\n",
        "                # Giving exact tags in training data a common parent tag for less complexity\n",
        "                # and more accuracy\n",
        "                if (tag == 'I-NP' or tag == 'B-NP' or tag == 'O'):\n",
        "                    tag = 'NN'\n",
        "\n",
        "                # Appending the tag to the list of tags\n",
        "                tags.append(tag)\n",
        "\n",
        "                # Calculating the number of times the tag and word appear together\n",
        "                emissionCountDict[tag + '|' + word] += 1\n",
        "\n",
        "                # Calculating the number of times that tag appears in general\n",
        "                separateTagCountDict[tag] += 1\n",
        "\n",
        "    # Iterating through the tag and word in the keys of emissionCountDict\n",
        "    for key in emissionCountDict:\n",
        "\n",
        "        # Getting the tag and word from the key\n",
        "        tagAndWord = key.split('|')\n",
        "        tag = tagAndWord[0]\n",
        "        word = tagAndWord[1]\n",
        "\n",
        "        # Calculating the emission probability\n",
        "        emissionProbability = (emissionCountDict[tag + '|' + word] + Lambda) / (separateTagCountDict[tag] + (Lambda * (22 ** 2)))\n",
        "\n",
        "        # Storing the emission probability in the dict\n",
        "        emissionProbabilityDict[word + '|' + tag] = emissionProbability\n",
        "\n",
        "    # Returning probability dict\n",
        "    return emissionProbabilityDict\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Words     : 396712\n",
            "Number of Sentences : 16993\n",
            "\n",
            "===========================\n",
            "PRINTING THE RESULTS\n",
            "===========================\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "साइबेरिया NN\n",
            "में PSP\n",
            "जनसँख्या JJ\n",
            "का PSP\n",
            "औसत JJ\n",
            "घनत्व PSP\n",
            "केवल RP\n",
            "4 QC\n",
            "व्यक्ति NN\n",
            "प्रति PSP\n",
            "वर्ग NN\n",
            "किलोमीटर NN\n",
            "है VM\n",
            "\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "उत्तरी JJ\n",
            "साइबेरिया NN\n",
            "बहुत QF\n",
            "सर्द JJ\n",
            "क्षेत्र NN\n",
            "है VM\n",
            "और CC\n",
            "यहाँ PRP\n",
            "गरमी NN\n",
            "का PSP\n",
            "मौसम NN\n",
            "केवल RP\n",
            "एक QC\n",
            "महीने NN\n",
            "रहता VM\n",
            "है VAUX\n",
            "\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "तुलना NN\n",
            "के PSP\n",
            "लिए PSP\n",
            "सन् XC\n",
            "2011 NN\n",
            "की PSP\n",
            "जनगणना NN\n",
            "में PSP\n",
            "भारत NNP\n",
            "के PSP\n",
            "बिहार NNP\n",
            "राज्य NN\n",
            "में PSP\n",
            "जन-घनत्व JJ\n",
            "1102 PSP\n",
            "व्यक्ति NN\n",
            "प्रति PSP\n",
            "वर्ग NN\n",
            "किमी NN\n",
            "था VM\n",
            "\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "मैं PRP\n",
            "थक VM\n",
            "गया VAUX\n",
            "हूँ VAUX\n",
            "\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "आपका PRP\n",
            "नाम NN\n",
            "क्या WQ\n",
            "है VM\n",
            "\n",
            "\n",
            "\n",
            "=========================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw6xYeTNdi7H"
      },
      "source": [
        "**Result of Improved Trigram**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgj_eXMDdpzH",
        "outputId": "b4dbd993-57a3-499e-f6a7-747ebcd4ea96"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import division\n",
        "import codecs\n",
        "from collections import defaultdict\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Opening the input training file\n",
        "    trainFile = codecs.open(\"trainDataHindi.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "    # Getting the emission probabilities\n",
        "    emissionProbabilityDict = calculateEmissionProbabilities(trainFile, 0.0, 0.00000000001)\n",
        "\n",
        "    # Opening the input training file\n",
        "    trainFile = codecs.open(\"trainDataHindi.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "    # Getting the transition probabilities and all the tags (states)\n",
        "    transitionProbabilityDict, allTags = calculateTransitionProbabilities(trainFile, 0.0, 9)\n",
        "\n",
        "    # Opening the input file\n",
        "    inputSentences = codecs.open(\"input.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "    # Initializing list for storing input sentences\n",
        "    sentencesList = []\n",
        "\n",
        "    # Reading the input file and storing the sentences\n",
        "    for line in inputSentences:\n",
        "        sentence = []\n",
        "        tokens = line.split()\n",
        "        for token in tokens:\n",
        "            word = token.split('|')[0].strip()\n",
        "            sentence.append(token)\n",
        "\n",
        "        # Appending the sentence in the list of sentences\n",
        "        sentencesList.append(sentence)\n",
        "\n",
        "    print(\"\\n===========================\\nPRINTING THE RESULTS\\n===========================\\n\")\n",
        "    # Iterating through each of the sentences\n",
        "    for sentence in sentencesList:\n",
        "\n",
        "        # Calling the algorithm on the sentence\n",
        "        predictedWordsAndTags = trigramHMMViterbiAlgorithm(sentence, allTags, emissionProbabilityDict, transitionProbabilityDict)\n",
        "        print(\"\\n=======\\n\")\n",
        "        # Printing the result of the algorithm\n",
        "        for (word, tag) in predictedWordsAndTags:\n",
        "            print(word, tag,)\n",
        "        print(\"\\n\")\n",
        "    print(\"\\n=========================\\n\")\n",
        "\n",
        "\n",
        "def trigramHMMViterbiAlgorithm(sentence, allTags, emissionProbabilityDict, transitionProbabilityDict):\n",
        "\n",
        "    # Appending two '*'s required for the initial trigram in the algorithm(base case)\n",
        "    sentence = [ u'*', u'*' ] + sentence\n",
        "\n",
        "    # Declaring the variables required\n",
        "    initialLength = len(sentence)\n",
        "    backtrackingDict = defaultdict(list)\n",
        "    dpDict = {}\n",
        "    tagsAssigned = []\n",
        "\n",
        "    # Initializing the base cases\n",
        "    dpDict[0, '*', '*'] = 1\n",
        "    for u in ([ u'*' ] + list(allTags)):\n",
        "        for v in ([ u'*' ] + list(allTags)):\n",
        "            if u != u'*' or v != u'*':\n",
        "                dpDict[0, u, v] = 1\n",
        "\n",
        "    # Iterating through each word in the sentence from starting in terms of length\n",
        "    for k in range(1, initialLength):\n",
        "        # For all possibilities of (w, u, v) tags\n",
        "        for v in allTags:\n",
        "            # For all possibilities of (w, u, v) tags\n",
        "            for u in allTags:\n",
        "\n",
        "                # List for possibilities for getting maximum probability\n",
        "                possibilities = []\n",
        "\n",
        "                # For all possibilities of (w, u, v) tags\n",
        "                for w in allTags:\n",
        "\n",
        "                    # Viterbi algorithm formula\n",
        "                    possibility = dpDict[k - 1 , w, u] * transitionProbabilityDict[(v, w, u)] * (emissionProbabilityDict[sentence[k] + '|' + v])\n",
        "\n",
        "                    # Appending the possibility to the list of possibilities\n",
        "                    possibilities.append((possibility, w))\n",
        "\n",
        "                # Getting the maximum probability from the list of possibilities, element[0] is the probability of the possibility\n",
        "                maxWUVgivenK = max(possibilities, key = lambda element:element[0])\n",
        "\n",
        "                # Storing the answer in the DP dict\n",
        "                dpDict[k, u, v] = maxWUVgivenK[0]\n",
        "\n",
        "                # Getting the element to be backtracked and storing it in the backtracking Dict\n",
        "                backtrackW = maxWUVgivenK[1]\n",
        "                backtrackingDict[u, v] += [backtrackW]\n",
        "\n",
        "                # Additionally storing (u, v) of (w, u, v) tags in the last iteration\n",
        "                if k == initialLength - 1:\n",
        "                    backtrackingDict[u, v] += [u, v]\n",
        "\n",
        "    # List for getting the possibilities of final iterations\n",
        "    maxPossibilitiesList = []\n",
        "\n",
        "    for key in dpDict:\n",
        "\n",
        "        # Getting the possibilities of the final iterations and appending to maxPossibilitiesList\n",
        "        if (initialLength - 1 in key) and dpDict[key]!=0:\n",
        "            maxPossibilitiesList.append((key[1:], dpDict[key]))\n",
        "\n",
        "    # Finally getting the answer of the viterbi alogrithm, element[1] is the probability of the final iteration possibilities\n",
        "    viterbiAnswer = max(maxPossibilitiesList, key = lambda element:element[1])\n",
        "\n",
        "    # Getting the tags associated with the viterbiAnswer by referring to the backtrackingDict\n",
        "    tagsAssigned = backtrackingDict[viterbiAnswer[0]][3:]\n",
        "\n",
        "    # Removing the two '*'s appended in the starting of algorithm, not required anymore\n",
        "    sentence = sentence[2:]\n",
        "\n",
        "    # Returning the answer\n",
        "    return zip(sentence, tagsAssigned)\n",
        "\n",
        "\n",
        "def calculateTransitionProbabilities(trainFile, Lambda, Weight):\n",
        "\n",
        "    # Declaring the variables and default dicts required\n",
        "    transitionProbabilityDict = defaultdict(int)\n",
        "    trigramTransitionCountDict = defaultdict(int)\n",
        "    bigramTransitionCountDict = defaultdict(int)\n",
        "    unigramTransitionCountDict = defaultdict(int)\n",
        "    numberOfWords = 0\n",
        "    numberOfSentences = 0\n",
        "    allTags = set([])\n",
        "\n",
        "    # Iterating through the lines of the input file\n",
        "    for line in trainFile.readlines():\n",
        "\n",
        "        # Getting tokens from each such line\n",
        "        tokens = line.split()\n",
        "\n",
        "        # Initializing a list for the tags observed in the line\n",
        "        tags = []\n",
        "\n",
        "        # For each token in that line\n",
        "        for token in tokens:\n",
        "\n",
        "            # Extracting the tag by splitting and stripping according to the file\n",
        "            tag = token.split('|')[2].split('.')[0].strip(':?').strip()\n",
        "\n",
        "            # Giving exact tags in training data a common parent tag for less complexity\n",
        "            # and more accuracy\n",
        "            if (tag == 'I-NP' or tag == 'B-NP' or tag == 'O'):\n",
        "                tag = 'NN'\n",
        "\n",
        "            allTags = allTags | set([tag])\n",
        "\n",
        "            # Appending the tag to the list of tags\n",
        "            tags.append(tag)\n",
        "\n",
        "        # Incrementing the total number of words by the required amount\n",
        "        numberOfWords += len(tags)\n",
        "\n",
        "        # If the line read is not a blank line\n",
        "        if tags != []:\n",
        "\n",
        "            # Incrementing the total number of sentences by the required amount\n",
        "            numberOfSentences+=1\n",
        "\n",
        "            # Getting the unigrams in the exact order\n",
        "            unigramSentenceList = tags[:]\n",
        "\n",
        "            # Getting the bigrams in the exact order by zipping through the tags\n",
        "            bigramSentenceList = zip(tags,tags[1:])\n",
        "\n",
        "            # Getting the trigrams in the exact order by zipping through the tags\n",
        "            trigramSentenceList = zip(tags, tags[1:], tags[2:])\n",
        "\n",
        "            # Incrementing the count of the unigrams as observed in the unigramSentenceList\n",
        "            for unigram in unigramSentenceList:\n",
        "                unigramTransitionCountDict[unigram] += 1\n",
        "\n",
        "            # Incrementing the count of the bigrams as observed in the bigramSentenceList\n",
        "            for bigram in bigramSentenceList:\n",
        "                bigramTransitionCountDict[bigram] += 1\n",
        "\n",
        "            # Incrementing the count of the trigrams as observed in the trigramSentenceList\n",
        "            for trigram in trigramSentenceList:\n",
        "                trigramTransitionCountDict[trigram] += 1\n",
        "\n",
        "    # Calculating the transition probabilities finally\n",
        "    # Iterating through the (u, v, s) trigrams\n",
        "    for trigram in trigramTransitionCountDict:\n",
        "\n",
        "        # Splitting each trigram into two ordered bigrams\n",
        "        # Initializing variables as required\n",
        "        uvBigram = trigram[:-1]\n",
        "        vsBigram = trigram[1:]\n",
        "        sState = (trigram[-1],)\n",
        "\n",
        "        # (u, v) bigram\n",
        "        uUnigram = uvBigram[0]\n",
        "        vstate = uvBigram[1]\n",
        "\n",
        "        # (v, s) bigram\n",
        "        vUnigram = vsBigram[0]\n",
        "        sstate = vsBigram[1]\n",
        "\n",
        "        # Calculating the bigram probabilities\n",
        "        transitionProbability1 = (bigramTransitionCountDict[uvBigram]) / (unigramTransitionCountDict[uUnigram])\n",
        "        transitionProbability2 = (bigramTransitionCountDict[vsBigram]) / (unigramTransitionCountDict[vUnigram])\n",
        "\n",
        "        # Declaring the q(s | u, v) - transition probability\n",
        "        keySgivenUV = sState + uvBigram\n",
        "\n",
        "        # Giving weights to repective bigram probabilities\n",
        "        # and using their combined effect as a trigram\n",
        "        # Calculating the transition probability\n",
        "        transitionProbability = (1 * transitionProbability2) + (Weight * transitionProbability1)\n",
        "\n",
        "        # Storing the transition probability in the dict\n",
        "        transitionProbabilityDict[keySgivenUV] = transitionProbability\n",
        "\n",
        "    # Printing the total number of words and sentences\n",
        "    print(\"Number of Words     : \" + str(numberOfWords))\n",
        "    print(\"Number of Sentences : \" + str(numberOfSentences))\n",
        "\n",
        "    # Returning probability dict and tags set\n",
        "    return transitionProbabilityDict, allTags\n",
        "\n",
        "\n",
        "def calculateEmissionProbabilities(trainFile, Lambda, offset):\n",
        "\n",
        "    # Declaring the variables and default dicts required\n",
        "    # 0.00000000001 as offset for smoothing\n",
        "    emissionProbabilityDict = defaultdict(lambda :  offset)\n",
        "    emissionCountDict = defaultdict(int)\n",
        "    separateTagCountDict = defaultdict(int)\n",
        "\n",
        "    # Iterating through the lines of the input file\n",
        "    for line in trainFile.readlines():\n",
        "\n",
        "        # Getting tokens form each such line\n",
        "        tokens = line.split()\n",
        "\n",
        "        # Initializing a list for the tags observed in the line\n",
        "        tags = []\n",
        "\n",
        "        # For each token in that line\n",
        "        for token in tokens:\n",
        "\n",
        "                # Extracting the word by splitting and stripping according to the file\n",
        "                word  = token.split('|')[0].strip()\n",
        "                # Extracting the tag by splitting and stripping according to the file\n",
        "                tag = token.split('|')[2].split('.')[0].strip(':?').strip()\n",
        "\n",
        "                # Giving exact tags in training data a common parent tag for less complexity\n",
        "                # and more accuracy\n",
        "                if (tag == 'I-NP' or tag == 'B-NP' or tag == 'O'):\n",
        "                    tag = 'NN'\n",
        "\n",
        "                # Appending the tag to the list of tags\n",
        "                tags.append(tag)\n",
        "\n",
        "                # Calculating the number of times the tag and word appear together\n",
        "                emissionCountDict[tag + '|' + word] += 1\n",
        "\n",
        "                # Calculating the number of times that tag appears in general\n",
        "                separateTagCountDict[tag] += 1\n",
        "\n",
        "    # Iterating through the tag and word in the keys of emissionCountDict\n",
        "    for key in emissionCountDict:\n",
        "\n",
        "        # Getting the tag and word from the key\n",
        "        tagAndWord = key.split('|')\n",
        "        tag = tagAndWord[0]\n",
        "        word = tagAndWord[1]\n",
        "\n",
        "        # Calculating the emission probability\n",
        "        emissionProbability = (emissionCountDict[tag + '|' + word] + Lambda) / (separateTagCountDict[tag] + (Lambda * (22 ** 2)))\n",
        "\n",
        "        # Storing the emission probability in the dict\n",
        "        emissionProbabilityDict[word + '|' + tag] = emissionProbability\n",
        "\n",
        "    # Returning probability dict\n",
        "    return emissionProbabilityDict\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Words     : 396712\n",
            "Number of Sentences : 16993\n",
            "\n",
            "===========================\n",
            "PRINTING THE RESULTS\n",
            "===========================\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "साइबेरिया NN\n",
            "में PSP\n",
            "जनसँख्या JJ\n",
            "का PSP\n",
            "औसत JJ\n",
            "घनत्व JJ\n",
            "केवल RP\n",
            "4 QC\n",
            "व्यक्ति NN\n",
            "प्रति PSP\n",
            "वर्ग NN\n",
            "किलोमीटर NN\n",
            "है VAUX\n",
            "\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "उत्तरी JJ\n",
            "साइबेरिया NN\n",
            "बहुत QF\n",
            "सर्द JJ\n",
            "क्षेत्र NN\n",
            "है VM\n",
            "और CC\n",
            "यहाँ PRP\n",
            "गरमी NN\n",
            "का PSP\n",
            "मौसम NN\n",
            "केवल RP\n",
            "एक QC\n",
            "महीने NN\n",
            "रहता VM\n",
            "है VAUX\n",
            "\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "तुलना NN\n",
            "के PSP\n",
            "लिए PSP\n",
            "सन् XC\n",
            "2011 XC\n",
            "की PSP\n",
            "जनगणना NN\n",
            "में PSP\n",
            "भारत XC\n",
            "के PSP\n",
            "बिहार JJ\n",
            "राज्य NN\n",
            "में PSP\n",
            "जन-घनत्व JJ\n",
            "1102 PSP\n",
            "व्यक्ति NN\n",
            "प्रति PSP\n",
            "वर्ग NN\n",
            "किमी NN\n",
            "था VAUX\n",
            "\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "मैं PRP\n",
            "थक VM\n",
            "गया VAUX\n",
            "हूँ VAUX\n",
            "\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "आपका PRP\n",
            "नाम NN\n",
            "क्या WQ\n",
            "है VM\n",
            "\n",
            "\n",
            "\n",
            "=========================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1d8sMsIQ2Jm",
        "outputId": "82ea0200-2991-4416-b74f-4840794b70eb"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adKIPMDti_QR"
      },
      "source": [
        "**Result of Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FSQUWy0vVMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "741e8dd6-1a22-4307-bd21-7654674539bc"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# Importing the libraries required\n",
        "import nltk, codecs\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# TRAINING DECISION TREE POS TAGGER IN HINDI USING THE ABOVE LIBRARIES\n",
        "\n",
        "# Initializing the classifier to be used\n",
        "classifier = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse = False)),\n",
        "    ('classifier', DecisionTreeClassifier(criterion = 'entropy'))\n",
        "])\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Opening training file\n",
        "    trainFile = codecs.open(\"trainDataHindi.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "    # Getting the nltk corpus treebank consisting of tagged sentences\n",
        "    taggedSentences = getTaggedSentences(trainFile)[:-10000]\n",
        "\n",
        "    # Printing the number of tagged sentences and words in the same\n",
        "    print(\"Number of tagged sentences in dataset : \" + str(len(taggedSentences)))\n",
        "    numberOfWords = 0\n",
        "    for sentence in taggedSentences:\n",
        "        numberOfWords += len(sentence)\n",
        "\n",
        "    print(\"Number of tagged words in dataset     : \" + str(numberOfWords))\n",
        "\n",
        "  \n",
        "\n",
        "    # Training 75% of tagged sentences as it is an ideal partition\n",
        "    cutoff = int(.75 * len(taggedSentences))\n",
        "\n",
        "    # Splitting the same to learn from 75% and then test on 25% of data\n",
        "    trainingSentences = taggedSentences[:cutoff]\n",
        "    testSentences = taggedSentences[cutoff:]\n",
        "\n",
        "    # Printing the number of tagged sentences and test sentences\n",
        "    print(\"The number of training sentences are  : \" + str(len(trainingSentences)))\n",
        "    print(\"The number of test sentences are      : \" + str(len(testSentences)))\n",
        "\n",
        "    # Tranforming to dataset to use inbuilt classifier function to train the model\n",
        "    X, y = transformToDataset(trainingSentences)\n",
        "\n",
        "    print(\"\\nPlease wait...Training the model.\\n\")\n",
        "\n",
        "    # Training(Fitting) the model\n",
        "    classifier.fit(X[:10000], y[:10000])\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "    # Tranforming to dataset to use inbuilt classifier function to train the model\n",
        "    XTest, yTest = transformToDataset(testSentences)\n",
        "\n",
        "    # Computing and printing the accuracy of the model\n",
        "    print(\"\\n===================\\n\")\n",
        "    print(\"ACCURACY : \" + str(classifier.score(XTest, yTest) * 100))\n",
        "    print(\"\\n===================\\n\")\n",
        "    # Opening the input file\n",
        "    inputSentences = codecs.open(\"input.txt\", mode = \"r\", encoding = \"utf-8\")\n",
        "\n",
        "    # Initializing list for storing input sentences\n",
        "    sentencesList = []\n",
        "\n",
        "    # Reading the input file and storing the sentences\n",
        "    for line in inputSentences:\n",
        "        sentence = []\n",
        "        tokens = line.split()\n",
        "        for token in tokens:\n",
        "            word = token.split('|')[0].strip()\n",
        "            sentence.append(token)\n",
        "\n",
        "        # Appending the sentence in the list of sentences\n",
        "        sentencesList.append(sentence)\n",
        "\n",
        "    print(\"\\n===========================\\nPRINTING THE RESULTS\\n===========================\\n\")\n",
        "    # Iterating through each of the sentences\n",
        "    for sentence in sentencesList:\n",
        "\n",
        "        # Calling the algorithm on the sentence\n",
        "        predictedWordsAndTags = posTag(sentence)\n",
        "        print(\"\\n=======\\n\")\n",
        "        # Printing the result of the algorithm\n",
        "        for (word, tag) in predictedWordsAndTags:\n",
        "            print(word, tag,)\n",
        "        print(\"\\n\")\n",
        "    print(\"\\n=========================\\n\")\n",
        "\n",
        "def features(sentence, index):\n",
        "    \"\"\"Function to return the features to be used in the model. Index is the index of the specific word in the sentence.\"\"\"\n",
        "\n",
        "    # Returning the necessary features applied to the word, i.e., sentence[index]\n",
        "    return {\n",
        "        'word': sentence[index],\n",
        "        'isFirst': index == 0,\n",
        "        'isLast': index == len(sentence) - 1,\n",
        "        'prefix1': sentence[index][0],\n",
        "        'prefix2': sentence[index][:2],\n",
        "        'prefix3': sentence[index][:3],\n",
        "        'suffix1': sentence[index][-1],\n",
        "        'suffix2': sentence[index][-2:],\n",
        "        'suffix3': sentence[index][-3:],\n",
        "        'previousWord': '' if index == 0 else sentence[index - 1],\n",
        "        'nextWord': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
        "        'hasHyphen': '-' in sentence[index],\n",
        "        'isNumeric': sentence[index].isdigit()\n",
        "    }\n",
        "\n",
        "def untag(taggedSentence):\n",
        "    \"\"\"Function to strip the tags from the sentences in our trained corpus and return the list of only words.\"\"\"\n",
        "    return [word for word, tag in taggedSentence]\n",
        "\n",
        "\n",
        "def transformToDataset(taggedSentences):\n",
        "    \"\"\"Function to tranform the tagged sentences into dataset suitable to pass into inbuilt classifier function.\"\"\"\n",
        "    # X is the list of word specific the features and y is the corresponding tags\n",
        "    X, y = [], []\n",
        "\n",
        "    # Appending the corresponding values of features and tags to X and y respectively\n",
        "    for taggedSentence in taggedSentences:\n",
        "        untaggedSentence = untag(taggedSentence)\n",
        "        for index in range(len(taggedSentence)):\n",
        "            X.append(features(untaggedSentence, index))\n",
        "            y.append(taggedSentence[index][1])\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def posTag(sentence):\n",
        "    tags = classifier.predict([features(sentence, index) for index in range(len(sentence))])\n",
        "    return zip(sentence, tags)\n",
        "\n",
        "\n",
        "def getTaggedSentences(trainFile):\n",
        "    \"\"\"Function to get the tagged sentences in the input file. Output is the list of sentences\n",
        "       with elements as word, tag tuples.\"\"\"\n",
        "\n",
        "    # Initializing all tags list as a set\n",
        "    allTags = set([])\n",
        "\n",
        "    # Initializing sentences list\n",
        "    sentencesList = []\n",
        "\n",
        "    # Iterating through the lines of the input file\n",
        "    for line in trainFile.readlines():\n",
        "\n",
        "        # Getting tokens from each such line\n",
        "        tokens = line.split()\n",
        "\n",
        "        sentence = []\n",
        "\n",
        "        # Initializing a list for the tags observed in the line\n",
        "        tags = []\n",
        "\n",
        "        # For each token in that line\n",
        "        for token in tokens:\n",
        "\n",
        "            word = token.split('|')[0].strip()\n",
        "            # Extracting the tag by splitting and stripping according to the file\n",
        "            tag = token.split('|')[2].split('.')[0].strip(':?').strip()\n",
        "\n",
        "            # Giving exact tags in training data a common parent tag for less complexity\n",
        "            # and more accuracy\n",
        "            if (tag == 'I-NP' or tag == 'B-NP' or tag == 'O'):\n",
        "                tag = 'NN'\n",
        "\n",
        "            # Appending (word, tag) tuple to the sentence\n",
        "            sentence.append((word, tag))\n",
        "\n",
        "            allTags = allTags | set([tag])\n",
        "\n",
        "            # Appending the tag to the list of tags\n",
        "            tags.append(tag)\n",
        "\n",
        "        if len(sentence) > 0:\n",
        "            # Appending the sentence to the list of sentences\n",
        "            sentencesList.append(sentence)\n",
        "\n",
        "    return sentencesList\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of tagged sentences in dataset : 6993\n",
            "Number of tagged words in dataset     : 163433\n",
            "The number of training sentences are  : 5244\n",
            "The number of test sentences are      : 1749\n",
            "\n",
            "Please wait...Training the model.\n",
            "\n",
            "Training completed.\n",
            "\n",
            "===================\n",
            "\n",
            "ACCURACY : 86.8003361748109\n",
            "\n",
            "===================\n",
            "\n",
            "\n",
            "===========================\n",
            "PRINTING THE RESULTS\n",
            "===========================\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "साइबेरिया NN\n",
            "में PSP\n",
            "जनसँख्या JJ\n",
            "का PSP\n",
            "औसत JJ\n",
            "घनत्व NN\n",
            "केवल RP\n",
            "4 QC\n",
            "व्यक्ति NN\n",
            "प्रति PSP\n",
            "वर्ग NN\n",
            "किलोमीटर WQ\n",
            "है SYM\n",
            "\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "उत्तरी JJ\n",
            "साइबेरिया NN\n",
            "बहुत QF\n",
            "सर्द NN\n",
            "क्षेत्र NN\n",
            "है VM\n",
            "और CC\n",
            "यहाँ PRP\n",
            "गरमी JJ\n",
            "का PSP\n",
            "मौसम NN\n",
            "केवल RP\n",
            "एक QC\n",
            "महीने NN\n",
            "रहता VM\n",
            "है SYM\n",
            "\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "तुलना NN\n",
            "के PSP\n",
            "लिए PSP\n",
            "सन् XC\n",
            "2011 QC\n",
            "की PSP\n",
            "जनगणना NN\n",
            "में PSP\n",
            "भारत NNP\n",
            "के PSP\n",
            "बिहार NNP\n",
            "राज्य XC\n",
            "में PSP\n",
            "जन-घनत्व QC\n",
            "1102 QC\n",
            "व्यक्ति NN\n",
            "प्रति PSP\n",
            "वर्ग NN\n",
            "किमी QF\n",
            "था SYM\n",
            "\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "मैं PRP\n",
            "थक NN\n",
            "गया VAUX\n",
            "हूँ SYM\n",
            "\n",
            "\n",
            "\n",
            "=======\n",
            "\n",
            "आपका NN\n",
            "नाम NN\n",
            "क्या WQ\n",
            "है SYM\n",
            "\n",
            "\n",
            "\n",
            "=========================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFo2jIuKsxcJ",
        "outputId": "7bfabce5-c514-4929-ceec-e0f265b644e1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('treebank')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R35o6BAmw7d"
      },
      "source": [
        "**Result of Improved Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMP_nZqym4vj",
        "outputId": "ef0893fe-8cd0-4038-8a87-2160e651e246"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# Importing the libraries required\n",
        "import nltk\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# TRAINING DECISION TREE POS TAGGER IN ENGLISH USING THE ABOVE LIBRARIES\n",
        "\n",
        "# Initializing the classifier to be used\n",
        "classifier = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse = False)),\n",
        "    ('classifier', DecisionTreeClassifier(criterion = 'entropy'))\n",
        "])\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Getting the nltk corpus treebank consisting of tagged sentences\n",
        "    taggedSentences = nltk.corpus.treebank.tagged_sents()\n",
        "\n",
        "    # Printing the number of tagged sentences and words in the same\n",
        "    print(\"Number of tagged sentences in dataset : \" + str(len(taggedSentences)))\n",
        "    print(\"Number of tagged words in dataset     : \" + str(len(nltk.corpus.treebank.tagged_words())))\n",
        "\n",
        "    # # Printing an example illustrating the features used in the model\n",
        "    # exampleFeatures = features(['This', 'is', 'a', 'sentence'], 2)\n",
        "    # for key in exampleFeatures:\n",
        "    #     print key,\n",
        "    #     print exampleFeatures[key]\n",
        "\n",
        "    # Training 75% of tagged sentences as it is an ideal partition\n",
        "    cutoff = int(.75 * len(taggedSentences))\n",
        "\n",
        "    # Splitting the same to learn from 75% and then test on 25% of data\n",
        "    trainingSentences = taggedSentences[:cutoff]\n",
        "    testSentences = taggedSentences[cutoff:]\n",
        "\n",
        "    # Printing the number of tagged sentences and test sentences\n",
        "    print(\"The number of training sentences are : \" + str(len(trainingSentences)))\n",
        "    print(\"The number of test sentences are     : \" + str(len(testSentences)))\n",
        "\n",
        "    # Tranforming to dataset to use inbuilt classifier function to train the model\n",
        "    X, y = transformToDataset(trainingSentences)\n",
        "\n",
        "    print(\"\\nPlease wait...Training the model.\\n\")\n",
        "\n",
        "    # Training(Fitting) the model\n",
        "    classifier.fit(X[:10000], y[:10000])\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "    # Tranforming to dataset to use inbuilt classifier function to train the model\n",
        "    XTest, yTest = transformToDataset(testSentences)\n",
        "\n",
        "    # Computing and printing the accuracy of the model\n",
        "    print(\"\\n===================\\n\")\n",
        "    print(\"ACCURACY : \" + str(classifier.score(XTest, yTest) * 100))\n",
        "    print(\"\\n===================\\n\")\n",
        "\n",
        "    # Printing the sentence to be POS tagged\n",
        "    sentence = \"My name is Parag. His name is Angshuman.\"\n",
        "    print(\"\\nTagging the sentence : \" + sentence + \"\\n\")\n",
        "\n",
        "    # Tagging the sentence using the trained model\n",
        "    print(posTag(nltk.word_tokenize(sentence)))\n",
        "\n",
        "def features(sentence, index):\n",
        "    \"\"\"Function to return the features to be used in the model. Index is the index of the specific word in the sentence.\"\"\"\n",
        "\n",
        "    # Returning the necessary features applied to the word, i.e., sentence[index]\n",
        "    return {\n",
        "        'word': sentence[index],\n",
        "        'isFirst': index == 0,\n",
        "        'isLast': index == len(sentence) - 1,\n",
        "        'isCapitalized': sentence[index][0].upper() == sentence[index][0],\n",
        "        'isAllCaps': sentence[index].upper() == sentence[index],\n",
        "        'isAllLower': sentence[index].lower() == sentence[index],\n",
        "        'prefix1': sentence[index][0],\n",
        "        'prefix2': sentence[index][:2],\n",
        "        'prefix3': sentence[index][:3],\n",
        "        'suffix1': sentence[index][-1],\n",
        "        'suffix2': sentence[index][-2:],\n",
        "        'suffix3': sentence[index][-3:],\n",
        "        'previousWord': '' if index == 0 else sentence[index - 1],\n",
        "        'nextWord': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
        "        'hasHyphen': '-' in sentence[index],\n",
        "        'isNumeric': sentence[index].isdigit(),\n",
        "        'capitalsInside': sentence[index][1:].lower() != sentence[index][1:]\n",
        "    }\n",
        "\n",
        "def untag(taggedSentence):\n",
        "    \"\"\"Function to strip the tags from the sentences in our trained corpus and return the list of only words.\"\"\"\n",
        "    return [word for word, tag in taggedSentence]\n",
        "\n",
        "\n",
        "def transformToDataset(taggedSentences):\n",
        "    \"\"\"Function to tranform the tagged sentences into dataset suitable to pass into inbuilt classifier function.\"\"\"\n",
        "    # X is the list of word specific the features and y is the corresponding tags\n",
        "    X, y = [], []\n",
        "\n",
        "    # Appending the corresponding values of features and tags to X and y respectively\n",
        "    for taggedSentence in taggedSentences:\n",
        "        untaggedSentence = untag(taggedSentence)\n",
        "        for index in range(len(taggedSentence)):\n",
        "            X.append(features(untaggedSentence, index))\n",
        "            y.append(taggedSentence[index][1])\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def posTag(sentence):\n",
        "    \"\"\"Function to return the tags as predicted by the model.\"\"\"\n",
        "    tags = classifier.predict([features(sentence, index) for index in range(len(sentence))])\n",
        "    return zip(sentence, tags)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of tagged sentences in dataset : 3914\n",
            "Number of tagged words in dataset     : 100676\n",
            "The number of training sentences are : 2935\n",
            "The number of test sentences are     : 979\n",
            "\n",
            "Please wait...Training the model.\n",
            "\n",
            "Training completed.\n",
            "\n",
            "===================\n",
            "\n",
            "ACCURACY : 89.55487706893781\n",
            "\n",
            "===================\n",
            "\n",
            "\n",
            "Tagging the sentence : My name is Parag. His name is Angshuman.\n",
            "\n",
            "<zip object at 0x7f60635f4d70>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}